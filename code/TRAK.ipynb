{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll provide a minimal and hackabale implementation of TRAK. For a more elaborate implementation with all the bells & whistles, see https://github.com/MadryLab/trak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# let's abstract away the \"boring\" parts in utils.py\n",
    "from utils import get_model, get_loader\n",
    "\n",
    "model = get_model()\n",
    "# we'll use a smaller training set containing only samples from the cat & dog classes\n",
    "train_loader = get_loader(split=\"train\", batch_size=500)\n",
    "val_loader = get_loader(split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a few models trained already (identical models trained on the same dataset, but with a different random seed). Let's load two of them.\n",
    "\n",
    "For convenience, we're also adding the code to train these models from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 2\n",
    "models = []\n",
    "for i in range(NUM_MODELS):\n",
    "    model = get_model()\n",
    "    sd = torch.load(f\"./artifacts/model_{i}.pt\")\n",
    "    model.load_state_dict(sd)\n",
    "    models.append(model)\n",
    "\n",
    "# training from scratch (in case you want to regenerate the above checkpoints yourself)\n",
    "want_to_retrain = False\n",
    "if want_to_retrain:\n",
    "    from utils import train\n",
    "    models = []\n",
    "    for i in range(NUM_MODELS):\n",
    "        model = get_model()\n",
    "        model = train(model, train_loader)\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the random projection matrix of size model_size x proj_dim in practice, this ends up being too large to work with, so we use a custom CUDA kernel (https://github.com/MadryLab/trak/tree/main/fast_jl) to project using this matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dim = 512\n",
    "model_size = sum(torch.numel(p) for p in model.parameters())\n",
    "P = torch.randn(model_size, proj_dim, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the attribution score from a few train samples to one test (target) sample. To this end, we need to compute the surrogate features of the train and target samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the derivation in the second part of the tutorial, we'll use the *loss* when we \"featurize\" the train sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae9766a404f4148bc4219485d5304d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5640dc2cb44457ba9c560b270872607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train = next(iter(train_loader))\n",
    "x_train = x_train.to(\"cuda\")\n",
    "y_train = y_train.to(\"cuda\")\n",
    "\n",
    "Phi_train = {i: [] for i in range(NUM_MODELS)}\n",
    "for i, model in enumerate(models):\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    L = loss(model(x_train), y_train)\n",
    "    for l in tqdm(L):  # iterate over the loss for each sample in the batch\n",
    "        phi = torch.autograd.grad(l, model.parameters(), retain_graph=True)\n",
    "        phi = torch.cat([p.flatten() for p in phi])  # flatten the gradients into a single vector\n",
    "        Phi_train[i].append((P.T @ phi).clone().detach())\n",
    "    Phi_train[i] = torch.stack(Phi_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and we'll use the model output (or \"measurement\") of interest when we \"featurize\" the target sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the first sample in the validation set as the target sample\n",
    "x_target, y_target = next(iter(val_loader))\n",
    "x_target = x_target[0:1].to(\"cuda\")\n",
    "y_target = y_target[0:1].to(\"cuda\")\n",
    "\n",
    "def model_output(logits, label):\n",
    "    \"\"\"\n",
    "    This function computes \"margins\", i.e. the difference between the logits of the target class and the log-sum-exp of the logits of all the other classes.\n",
    "    \"\"\"\n",
    "    bindex = torch.arange(logits.shape[0]).to(logits.device, non_blocking=False)\n",
    "    logits_correct = logits[bindex, label.unsqueeze(0)]\n",
    "\n",
    "    cloned_logits = logits.clone()\n",
    "    # remove the logits of the correct labels from the sum\n",
    "    # in logsumexp by setting to -ch.inf\n",
    "    cloned_logits[bindex, label.unsqueeze(0)] = torch.tensor(\n",
    "        -float(\"inf\"), device=logits.device, dtype=logits.dtype\n",
    "    )\n",
    "\n",
    "    margins = logits_correct - cloned_logits.logsumexp(dim=-1)\n",
    "    return margins.sum()\n",
    "\n",
    "Phi_target = {}\n",
    "for i, model in enumerate(models):\n",
    "    O = model_output(model(x_target), y_target)\n",
    "    phi = torch.autograd.grad(O, model.parameters(), create_graph=True)\n",
    "    phi = torch.cat([p.flatten() for p in phi])  # flatten the gradients into a single vector\n",
    "    Phi_target[i] = (P.T @ phi).clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to compute an estimate of the Hessian matrix. It turns out that for our linear surrogate model, the Hessian has a simple closed form!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = {i: torch.zeros(proj_dim, proj_dim, device=\"cuda\") for i in range(2)}\n",
    "for i, model in enumerate(models):\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        L = loss(model(x), y)\n",
    "        phi = torch.autograd.grad(L, model.parameters(), create_graph=True)\n",
    "        phi = torch.cat([p.flatten() for p in phi])\n",
    "        X = (P.T @ phi.reshape(-1, 1)).clone().detach()\n",
    "        H[i] += X @ X.T\n",
    "\n",
    "# we can optionally add a damping term lambda * I here\n",
    "H_inv = {i: torch.linalg.inv(H[i]) for i in range(2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to compute our attribution scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.zeros(Phi_train[0].shape[0])\n",
    "for k in Phi_train.keys():\n",
    "    scores += (Phi_train[k] @ H_inv[k] @ Phi_target[k] / len(Phi_train)).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

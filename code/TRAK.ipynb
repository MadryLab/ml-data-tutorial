{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll provide a minimal and hackabale implementation of TRAK. For a more elaborate implementation with all the bells&whistles, see https://github.com/MadryLab/trak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# let's abstract away the \"boring\" parts in utils.py\n",
    "from utils import get_model, get_loader\n",
    "\n",
    "model = get_model()\n",
    "train_loader = get_loader(split=\"train\")\n",
    "val_loader = get_loader(split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a few models trained already (identical models trained on the same dataset, but with a different random seed). Let's load two of them; below we'll load pre-trained checkpoints.\n",
    "\n",
    "For convenience, we're also adding the code to train these models from scratch (commented out below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 2\n",
    "models = []\n",
    "\n",
    "# from utils import train\n",
    "# for i in range(NUM_MODELS):\n",
    "#     model = get_model()\n",
    "#     model = train(model, train_loader)\n",
    "#     models.append(model)\n",
    "\n",
    "for path in [\"./models/model_0.pt\", \"./models/model_1.pt\"]:\n",
    "    model = get_model()\n",
    "    sd = torch.load(path)\n",
    "    model.load_state_dict(sd)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the random projection matrix of size model_size x proj_dim in practice, this ends up being too large to work with, so we use a custom CUDA kernel (https://github.com/MadryLab/trak/tree/main/fast_jl) to project using this matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dim = 512\n",
    "model_size = sum(torch.numel(p) for p in model.parameters())\n",
    "P = torch.randn(model_size, proj_dim, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the attribution score from one train sample to one test (target) sample. To this end, we need to compute the surrogate features of the train and target samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the derivation in the second part of the tutorial, we'll use the *loss* when we \"featurize\" the train sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_loader.dataset[0]\n",
    "x = x.to(\"cuda\")\n",
    "y = torch.tensor(y).to(torch.int64).to(\"cuda\")\n",
    "\n",
    "Phi_train = {}\n",
    "for i, model in enumerate(models):\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    L = loss(model(x.unsqueeze(0)), y.unsqueeze(0))\n",
    "    phi = torch.autograd.grad(L, model.parameters(), create_graph=True)\n",
    "    phi = torch.cat([p.flatten() for p in phi])  # flatten the gradients into a single vector\n",
    "    Phi_train[i] = (P.T @ phi).clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and we'll use the model output (or \"measurement\") of interest when we \"featurize\" the target sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_target, y_target = val_loader.dataset[0]\n",
    "def model_output(logits, label):\n",
    "    \"\"\"\n",
    "    This function computes \"margins\", i.e. the difference between the logits of the target class and the log-sum-exp of the logits of all the other classes.\n",
    "    \"\"\"\n",
    "    bindex = torch.arange(logits.shape[0]).to(logits.device, non_blocking=False)\n",
    "    logits_correct = logits[bindex, label.unsqueeze(0)]\n",
    "\n",
    "    cloned_logits = logits.clone()\n",
    "    # remove the logits of the correct labels from the sum\n",
    "    # in logsumexp by setting to -ch.inf\n",
    "    cloned_logits[bindex, label.unsqueeze(0)] = torch.tensor(\n",
    "        -float(\"inf\"), device=logits.device, dtype=logits.dtype\n",
    "    )\n",
    "\n",
    "    margins = logits_correct - cloned_logits.logsumexp(dim=-1)\n",
    "    return margins.sum()\n",
    "\n",
    "Phi_target = {}\n",
    "for i, model in enumerate(models):\n",
    "    O = model_output(model(x.unsqueeze(0)), y.unsqueeze(0))\n",
    "    phi = torch.autograd.grad(O, model.parameters(), create_graph=True)\n",
    "    phi = torch.cat([p.flatten() for p in phi])  # flatten the gradients into a single vector\n",
    "    Phi_target[i] = (P.T @ phi).clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to compute an estimate of the Hessian matrix. It turns out that for our linear surrogate model, the Hessian has a simple closed form!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_775693/2949813731.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y).to(torch.int64).to(\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "H = {i: torch.zeros(proj_dim, proj_dim, device=\"cuda\") for i in range(2)}\n",
    "for i, model in enumerate(models):\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(\"cuda\")\n",
    "        y = torch.tensor(y).to(torch.int64).to(\"cuda\")\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        L = loss(model(x), y)\n",
    "        phi = torch.autograd.grad(L, model.parameters(), create_graph=True)\n",
    "        phi = torch.cat([p.flatten() for p in phi])\n",
    "        X = (P.T @ phi.reshape(-1, 1)).clone().detach()\n",
    "        H[i] += X @ X.T\n",
    "\n",
    "# we can optionally add a damping term lambda * I here\n",
    "H_inv = {i: torch.linalg.inv(H[i]) for i in range(2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, we are ready to compute our attribution scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-164915.9375, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "score = 0.\n",
    "for k in Phi_train.keys():\n",
    "    score += Phi_train[k] @ H_inv[k] @ Phi_target[k] / len(Phi_train)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

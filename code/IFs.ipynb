{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a simple example of how to compute influence functions\n",
    "for deep neural networks, following the method described in [[KH17]](https://proceedings.mlr.press/v70/koh17a.html). We have\n",
    "\n",
    "based our implementation on [this\n",
    "repositoty](https://github.com/alstonlo/torch-influence/), and have introduced\n",
    "only minor changes aimed at highlighting the main idea behind the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/xfs/home/krisgrg/projects/tutorial-md/code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/xfs/home/krisgrg/conda_envs/tutorial/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/xfs/home/krisgrg/projects/tutorial-md/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.sparse.linalg as L\n",
    "\n",
    "\n",
    "# let's abstract away the \"boring\" parts in utils.py\n",
    "from utils import get_model, get_loader\n",
    "\n",
    "model = get_model()\n",
    "# we'll use a smaller training set containing only samples from the cat & dog classes\n",
    "train_loader = get_loader(split=\"train\", batch_size=500)\n",
    "val_loader = get_loader(split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a trained model already. Let's load it.\n",
    "\n",
    "For convenience, we're also adding the code to train this model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.load(\"./artifacts/model_0.pt\")\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "# training from scratch (in case you want to regenerate the above checkpoints yourself)\n",
    "want_to_retrain = False\n",
    "if want_to_retrain:\n",
    "    from utils import train\n",
    "    model = train(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to compute the gradient wrt the loss of our target example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the first sample in the validation set as the target sample\n",
    "x_target, y_target = next(iter(val_loader))\n",
    "x_target = x_target[0:1].to(\"cuda\")\n",
    "y_target = y_target[0:1].to(\"cuda\")\n",
    "\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "l = loss(model(x_target), y_target)\n",
    "phi = torch.autograd.grad(l, model.parameters(), retain_graph=True)\n",
    "phi = torch.cat([p.flatten() for p in phi])  # flatten the gradients into a single vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes the critical part: we will compute the matrix-vector product `stest` (using the naming convention introduced in [KH17]) via the conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = self._model_make_functional()\n",
    "damp = 0.001  # damping factor, i.e., regularization for the Hessian\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "flat_params = torch.cat([p.flatten() for p in model.parameters()])\n",
    "\n",
    "def hvp_fn(v):\n",
    "    v = torch.tensor(v, requires_grad=False, device=\"cuda\")\n",
    "\n",
    "    hvp = 0.0\n",
    "    for batch in train_loader:\n",
    "        def f(params_):\n",
    "            return loss(model(batch[0].to(\"cuda\")), batch[1].to(\"cuda\")) + 1e-4 * torch.square(params_.norm())\n",
    "        hvp_batch = torch.autograd.functional.hvp(f, flat_params, v)[1]\n",
    "        batch_size = batch[0].shape[0]\n",
    "        hvp = hvp + hvp_batch.detach() * batch_size\n",
    "\n",
    "    hvp = hvp / len(train_loader.dataset)\n",
    "    hvp = hvp + damp * v\n",
    "\n",
    "    return hvp.cpu().numpy()\n",
    "\n",
    "d = phi.shape[0]\n",
    "linop = L.LinearOperator((d, d), matvec=hvp_fn)\n",
    "stest = L.cg(A=linop, b=phi.cpu().numpy(), atol=1e-8, maxiter=1000)[0]\n",
    "stest = torch.from_numpy(stest).to(torch.float32).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to calculate the gradients of the training samples and compute the influence function using them, together with `stest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e796e04d2e4a04b6a1b81ba24713ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "train_loader_single_samples = get_loader(split=\"train\", batch_size=1, indices=list(range(500)))\n",
    "for batch in tqdm(train_loader_single_samples):\n",
    "    outputs = model(batch[0].to(\"cuda\"))\n",
    "    l = loss(outputs, batch[1].to(\"cuda\")) + 1e-4 * torch.square(flat_params.norm())\n",
    "    grad_z = torch.autograd.grad(l, flat_params)[0]\n",
    "    scores.append(grad_z @ stest)\n",
    "\n",
    "scores = torch.stack(scores)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
